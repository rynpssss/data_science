{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配ブースティング決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  特徴\n",
    "\n",
    "- ランダムフォレストのポイント制で、総合ポイントにて判断\n",
    "- ランダムフォレストの課題を解決するために、より分岐した方を重みを高くする\n",
    "- ポイント計算に勾配を使っている\n",
    "- メモ　https://qiita.com/triwave33/items/aad60f25485a4595b5c8\n",
    "    - 決定木を直列につないで再学習を繰り返す\n",
    "    - 損失関数を最適する手法が分かっていない　※確率的勾配効果法は使えないらしい・・・\n",
    "    \n",
    "- バイアス（予測値と実測値の誤差の平均）\n",
    "- バリアンス（分散　予測値の散らばり度合い）\n",
    "- 説明可能性\n",
    "\t- 決定木　→　理解○\n",
    "\t- ランダムフォレスト　→　理解×　※木1つはわかるが、総合評価が分からない\n",
    "\t\t- 購買決定木　→　理解×　※ランダムフォレストの重み版なので分からない\n",
    "- バギングとアダブースト\n",
    "\t- バギング\n",
    "\t\t- ランダムフォレストのように、複数の決定木（識別器）を用意して、多数決で決める\n",
    "\t- アダブースト\n",
    "\t\t- バギングとは異なり、直列に決定木を準備する。\n",
    "\t\t- 正解データの重みを小さく、不正解データの重みを大きくすることで、より誤り率が少ないデータが作られる\n",
    "        \n",
    "***\n",
    "        \n",
    "## 理論\n",
    "\n",
    "    - 勾配降下法\n",
    "        - 最急降下法\n",
    "            - パラメータを学習率分変化させて、微分（傾き）０となる点を最適なパラメータとして見つける方法\n",
    "            - 問題点\n",
    "                - 全てのデータを利用して誤差を算出しているため、データが増えれば計算コストが増加（パラメータを更新するたびに再計算）\n",
    "                - 初期値依存問題によって、極小値にたどり着く可能性がある。\n",
    "        - 確率的勾配降下法\n",
    "            - 学習データからランダムに1セットだけ取り出して損失を計算\n",
    "            - メリット\n",
    "                - 1データのみなので計算コストが少ない\n",
    "                - 極小値にたどり着くことが少ない（1つのデータしか使っていないので、次に使うデータでは極小値にはならない可能性が高い）\n",
    "        - 学習率・損失関数・パラメータを組み合わせて、適切なパラメータを発見する\n",
    "    - 誤差を求める\n",
    "    - 損失関数から新しい値を計算（予測　＋　誤差×学習率）\n",
    "    - 再度、予測を行う（以下、ループ）\n",
    "  \n",
    "****  \n",
    "  \n",
    "## 使用モデル\n",
    "-  LightGBMとXGBoostの違い\n",
    "    - LightGBM\n",
    "        - 計算コストが少ない（特徴量をヒストグラムで階級的に持っているため）\n",
    "        - levev-wiseではなく、leaf-wiseを採用\n",
    "        - 精度が高い（leaf-wiseの方が、より複雑な決定木のため）\n",
    "        - 過学習しやすい（leaf-wiseが複雑なため）\n",
    "    - XGBoost\n",
    "        - 過学習しにくい（leave-wise）\n",
    "        - 精度はデータセットにより異なるが、LightGBMの方が良い場合が多い（複雑な決定木を作るため）\n",
    "        - パラメータの種類\n",
    "            - どのような決定木を作るか\n",
    "                - max_depth：木の根の最大値\n",
    "                - num_leaves：葉の最大数\n",
    "\n",
    "            - どういった目的関数にするか\n",
    "                - lambda：L2正則化項（過学習を避けるために行う）→　L1と違い、大きくしても特徴量に大きな影響がない。むしろ過学習を防ぐ。\n",
    "                - alpha：L1正則化項（高次元の場合に用いる）→　大きくしすぎると重要な特徴量以外は０になるので注意\n",
    "                - eta：勾配降下法のような学習率のパラメータ（小さくする事で、頑健性あるモデル）\n",
    "            - どうやって学習していくか\n",
    "                - learning_rate："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不明点\n",
    "### 21/12/29時点\n",
    " - なぜ、結構汎用的なモデルなのか？\n",
    " - どのような計算をしているか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関係性\n",
    "### 21/12/29時点\n",
    " -　現時点ではわからない（なんか、めっちゃ汎用性が高くて、予測精度も高い、最強モデルのイメージww）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
